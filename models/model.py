import torch
import torch.nn as nn
import torch.nn.functional as F

from models.encoder import separate_encoder, separate_encoder_layer
from models.decoder import Decoder, DecoderLayer
from models.attn import FullAttention, AttentionLayer
from models.embed import DataEmbedding
from models.simple_linear import simple_linear
from models.encoder import series_decomp
from fft_trans import fft_ifft_picture
from models.distribution_block import distribution_block
from models.fft_decompose import fft_decompose
from models.fft_plus import fft_plus
from draw_picture import draw_picture, draw_trend_res

class separateformer(nn.Module):
    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,
                 factor=5, d_model=512, n_heads=8,
                 dropout=0.05, attn='full', embed='fixed', freq='h', activation='gelu',
                 output_attention=False, mix=True, separate_factor=2, step=4,
                 device=torch.device('cuda:0')):
        super(separateformer,self).__init__()
        self.seq_len = seq_len
        self.pred_len = out_len #预测序列长度
        self.label_len = label_len
        self.attn = attn #attn模块选取
        self.output_attention = output_attention
        self.separate_factor = separate_factor
        self.dropout = dropout
        self.step = step
        #self.activation = F.gelu if activation == 'gelu' else F.relu
        self.activation = F.elu
        self.d_model = d_model
        self.c_out = c_out

        #encoding ETT中enc_in dec_in都为7 d_model为512，即把七个多元变量通过线性映射到512维上
        self.enc_embedding = DataEmbedding(enc_in,d_model, embed, freq, dropout)
        self.dec_embedding = DataEmbedding(dec_in,d_model,embed,freq,dropout)
        #true encoder 2layer
        self.encoder = separate_encoder(self.step, separate_factor=separate_factor,n_heads=n_heads, mix=mix, dropout=dropout, activation=activation, d_model=d_model)
        self.encoder2 = separate_encoder(self.step, separate_factor=separate_factor,n_heads=n_heads, mix=mix, dropout=dropout, activation=activation, d_model=d_model)
        #pred encoder  1layer
        self.encoder_pred = separate_encoder(self.step, separate_factor=separate_factor,n_heads=n_heads, mix=mix, dropout=dropout, activation=activation, d_model=d_model)
        #pred decoder  1layer
        self.decoder = Decoder(self.seq_len, self.label_len, self.pred_len, self.step, self.separate_factor, n_heads, mix,
                               self.dropout, self.d_model, self.c_out, self.activation)
        # linear extract periodical time series
        self.trade_off1 = nn.Parameter(torch.zeros(1, 1, c_out))
        self.trade_off2 = nn.Parameter(torch.ones(1, 1, c_out))

        self.simple_layer = simple_linear(input_dim=self.seq_len, output_dim=self.pred_len + self.label_len)
        self.decompose = series_decomp(seq_len * 3 // 4 + 1) # seq_len * 3 // 4 + 1

        self.dis_block = True
        if self.dis_block:
            self.distribution_block = distribution_block(seq_len=self.seq_len, label_len=self.label_len, pred_len=self.pred_len, feature_dim=c_out, window_size=12)
            self.distribution_weight = nn.Parameter(torch.ones(1, 1, c_out))
            self.distribution_bias = nn.Parameter(torch.zeros(1, 1, c_out))

        self.RIN = True
        if self.RIN:
            self.affine_weight = nn.Parameter(torch.ones(1, 1, c_out))
            self.affine_bias = nn.Parameter(torch.zeros(1, 1, c_out))
            self.affine_weight2 = nn.Parameter(torch.ones(1, 1, c_out))
            self.affine_bias2 = nn.Parameter(torch.zeros(1, 1, c_out))

        self.fft_plus = fft_plus(self.pred_len + self.label_len)


    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,enc_self_mask=None):
        """
        :param x_enc: encoder的输入[batch_size, sequence_len, c_in=7]
        :param x_mark_enc: 输入的时间戳[batche_size, sequence_len, 4]
        :param enc_self_mask:
        :return:
        """

        # dec_out_mean[batch_size, 1, d_model]
        dec_out_mean = torch.mean(x_dec[:, :self.label_len, :], dim=1).view(x_dec.shape[0], 1, x_dec.shape[2])
        # # temp[batch_size, pred_len, d_model]
        temp = dec_out_mean.repeat(1, self.pred_len, 1)
        # temp = torch.ones([x_dec.shape[0], self.pred_len, x_dec.shape[2]]).to(x_dec.device)
        # # dec_out中占位符为已知部分的均值或0 作为预测部分的encoder的输入
        x_dec = x_dec[:, :self.label_len, :]
        # x_dec = x_enc[:, -self.label_len:, :]
        x_dec = torch.cat([x_dec, temp], dim=1)
        x_enc_res, x_enc_trend = self.decompose(x_enc)
        # x_enc_trend, x_enc_res = fft_decompose(x_enc)
        # x_enc_trend = x_enc
        output_linear = self.simple_layer(x_enc_res)
        # draw_picture(x_enc_trend, x_enc_trend, 'decompose trend', 'decompose')
        # draw_picture(x_enc_res, x_enc_res, 'decompose res', 'decompose')

        if self.RIN:
            print('/// RIN ACTIVATED ///\r', end='')
            means1 = x_enc_trend.mean(1, keepdim=True).detach()
            #mean
            x_enc_trend = x_enc_trend - means1
            #var
            stdev1 = torch.sqrt(torch.var(x_enc_trend, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc_trend /= stdev1
            # affine
            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)
            x_enc_trend = x_enc_trend * self.affine_weight2 + self.affine_bias2

            means = x_dec.mean(1, keepdim=True).detach()
            # mean
            x_dec = x_dec - means
            # var
            stdev = torch.sqrt(torch.var(x_dec, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_dec /= stdev
            # affine
            # print(x.shape,self.affine_weight.shape,self.affine_bias.shape)
            x_dec = x_dec * self.affine_weight + self.affine_bias

        if self.dis_block:
            x_enc_trend, x_dec, label_pred_mean, lable_pred_std, pred_mean, pred_std = self.distribution_block(x_enc_trend, x_dec)
            x_dec = x_dec * self.distribution_weight + self.distribution_bias

        # first embedding
        enc_out = self.enc_embedding(x_enc_trend, x_mark_enc)
        dec_out = self.dec_embedding(x_dec, x_mark_dec)

        enc_out, layer_output_true = self.encoder(enc_out, attn_mask=enc_self_mask)
        #pred encoder
        dec_out, layer_output_pred = self.encoder_pred(dec_out, attn_mask=enc_self_mask)
        #decoder
        output = self.decoder(enc_out, dec_out, layer_output_true, layer_output_pred)

        if self.dis_block:
            output = output - self.distribution_bias
            output = output / (self.distribution_weight + 1e-10)
            output = output * lable_pred_std
            output = output + label_pred_mean

        ### reverse RIN ###
        if self.RIN:
            output = output - self.affine_bias
            output = output / (self.affine_weight + 1e-10)
            output = output * stdev
            output = output + means

        # draw_picture(output[:, self.label_len:, :], output[:, self.label_len:, :], 'decompose trend pred', 'decompose')
        # draw_picture(output_linear[:, self.label_len:, :], output_linear[:, self.label_len:, :], 'decompose res pred', 'decompose')
        output_final = output + self.trade_off2 * output_linear
        # output = output + self.trade_off1 * self.fft_plus(output)

        # true = [-1.249821700218765, -1.249821700218765, -1.249821700218765, -1.2429976012044277, -1.2338988025186448,
        #         -1.2270747035043077, -1.2270747035043077, -1.2270747035043077, -1.2248000038328621, -1.2088771061327421,
        #         -1.2088771061327421, -1.2088771061327421, -1.2179759048185248, -1.2270747035043077, -1.2270747035043077,
        #         -1.2293494031757535, -1.2384482018615361, -1.247547000547319, -1.249821700218765, -1.247547000547319,
        #         -1.2520963998902106, -1.2520963998902106, -1.2452723008758735, -1.2429976012044277, -1.2407229015329821,
        #         -1.2429976012044277, -1.2429976012044277, -1.2452723008758735, -1.247547000547319, -1.2316241028471993,
        #         -1.2361735021900906, -1.2520963998902106, -1.2543710995616562, -1.2589204989045477, -1.2611951985759935,
        #         -1.263469898247439, -1.2611951985759935, -1.263469898247439, -1.2657445979188848, -1.2657445979188848,
        #         -1.2611951985759935, -1.256645799233102, -1.2543710995616562, -1.2543710995616562, -1.249821700218765,
        #         -1.2543710995616562, -1.256645799233102, -1.2589204989045477, -1.2680192975903306, -1.272568696933222,
        #         -1.2816674956190048, -1.2930409939762333, -1.2975903933191248, -1.2930409939762333, -1.2930409939762333,
        #         -1.2907662943047875, -1.2930409939762333, -1.2930409939762333, -1.2930409939762333, -1.2953156936476788,
        #         -1.2998650929905704, -1.288491594633342, -1.2862168949618962, -1.2771180962761133, -1.2748433966046675,
        #         -1.2657445979188848, -1.2611951985759935, -1.256645799233102, -1.2589204989045477, -1.2543710995616562,
        #         -1.247547000547319, -1.2361735021900906, -1.2338988025186448, -1.2225253041614164, -1.2179759048185248,
        #         -1.2088771061327421, -1.2066024064612964, -1.202053007118405, -1.1975036077755135, -1.1838554097468394,
        #         -1.1838554097468394, -1.186130109418285, -1.186130109418285, -1.179306010403948, -1.1747566110610566,
        #         -1.170207211718165, -1.1588337133609365, -1.156559013689491, -1.147460215003708, -1.1383614163179252,
        #         -1.1383614163179252, -1.1292626176321423, -1.1292626176321423, -1.1224385186178052, -1.115614419603468,
        #         -1.108790320589131, -1.1019662215747938, -1.092867422889011, -1.092867422889011, -1.0883180235461196,
        #         -1.0814939245317825, -1.0814939245317825, -1.0746698255174454, -1.0746698255174454, -1.079219224860337,
        #         -1.083768624203228, -1.083768624203228, -1.083768624203228, -1.083768624203228, -1.083768624203228,
        #         -1.0860433238746738, -1.0860433238746738, -1.0951421225604567, -1.0974168222319025, -1.108790320589131,
        #         -1.115614419603468, -1.1201638189463594, -1.1224385186178052, -1.1224385186178052, -1.1178891192749139,
        #         -1.1133397199320223, -1.1133397199320223, -1.1178891192749139, -1.1178891192749139, -1.108790320589131,
        #         -1.1065156209176852, -1.1110650202605767, -1.1247132182892507, -1.1406361159893708, -1.1656578123752737,
        #         -1.1815807100753934, -1.186130109418285, -1.2043277067898508, -1.2134265054756337, -1.2043277067898508,
        #         -1.202053007118405, -1.1884048090897308, -1.1747566110610566, -1.1679325120467194, -1.1656578123752737,
        #         -1.161108413032382, -1.1542843140180452, -1.1520096143465994, -1.1542843140180452, -1.1633831127038279,
        #         -1.1679325120467194, -1.1679325120467194, -1.1679325120467194, -1.170207211718165, -1.170207211718165,
        #         -1.1679325120467194, -1.1656578123752737, -1.1588337133609365, -1.1520096143465994, -1.1497349146751537,
        #         -1.147460215003708, -1.1451855153322623, -1.1429108156608165, -1.1406361159893708, -1.1383614163179252,
        #         -1.1406361159893708, -1.1360867166464796, -1.1406361159893708, -1.1360867166464796, -1.1383614163179252,
        #         -1.1451855153322623, -1.1497349146751537, -1.1520096143465994, -1.1520096143465994, -1.1520096143465994,
        #         -1.156559013689491, -1.1633831127038279, -1.170207211718165, -1.1815807100753934, -1.1906795087611763,
        #         -1.1884048090897308, -1.1929542084326223, -1.1975036077755135, -1.1997783074469592, -1.1997783074469592,
        #         -1.2043277067898508, -1.2066024064612964, -1.2066024064612964, -1.202053007118405, -1.1975036077755135,
        #         -1.195228908104068, -1.1997783074469592, -1.1975036077755135, -1.1997783074469592, -1.195228908104068,
        #         -1.1975036077755135, -1.1906795087611763, -1.1884048090897308, -1.1884048090897308, -1.186130109418285,
        #         -1.1906795087611763, -1.1906795087611763, -1.186130109418285, -1.179306010403948, -1.1747566110610566,
        #         -1.1656578123752737, -1.161108413032382, -1.1542843140180452, -1.1520096143465994, -1.1497349146751537,
        #         -1.147460215003708, -1.1429108156608165, -1.1406361159893708, -1.1383614163179252, -1.1429108156608165,
        #         -1.147460215003708, -1.147460215003708, -1.1520096143465994, -1.1451855153322623, -1.1383614163179252,
        #         -1.1338120169750336, -1.1247132182892507, -1.1247132182892507, -1.1224385186178052, -1.115614419603468,
        #         -1.1110650202605767, -1.1019662215747938, -1.0974168222319025, -1.1019662215747938, -1.0951421225604567,
        #         -1.0883180235461196, -1.076944525188891, -1.0723951258459996, -1.0723951258459996, -1.0610216274887712,
        #         -1.0450987297886511, -1.0428240301172056, -1.0405493304457598, -1.0291758320885311, -1.0291758320885311,
        #         -1.0200770334027482, -1.0359999311028683, -1.031450531759977, -1.031450531759977, -1.0291758320885311
        #         ]
        # label = true[:48]
        # true = true[-192:]
        #
        # # ACDN
        # y1 = [-1.2504554986953735, -1.2609286308288574, -1.2519557476043701, -1.2453795671463013, -1.2338953018188477,
        #       -1.2279366254806519, -1.2289583683013916, -1.2252373695373535, -1.2223588228225708, -1.2122770547866821,
        #       -1.2089271545410156, -1.2043678760528564, -1.2153416872024536, -1.2272647619247437, -1.2265828847885132,
        #       -1.2328213453292847, -1.2367734909057617, -1.2463462352752686, -1.2456049919128418, -1.245986819267273,
        #       -1.250397801399231, -1.2549158334732056, -1.2513418197631836, -1.2496552467346191, -1.244442105293274,
        #       -1.2417986392974854, -1.2465604543685913, -1.246713638305664, -1.2497600317001343, -1.2356586456298828,
        #       -1.2386974096298218, -1.250835657119751, -1.2539122104644775, -1.2578935623168945, -1.2566007375717163,
        #       -1.253580093383789, -1.2558600902557373, -1.2646914720535278, -1.2642918825149536, -1.2605594396591187,
        #       -1.254407525062561, -1.2612779140472412, -1.2551332712173462, -1.2569962739944458, -1.2508841753005981,
        #       -1.2575764656066895, -1.2580602169036865, -1.262213945388794, -1.2632713317871094, -1.2575979232788086,
        #       -1.2557066679000854, -1.2530694007873535, -1.2505743503570557, -1.247373342514038, -1.245255708694458,
        #       -1.2430254220962524, -1.237849235534668, -1.2325233221054077, -1.2288709878921509, -1.2236469984054565,
        #       -1.218414306640625, -1.2139921188354492, -1.2120939493179321, -1.2077698707580566, -1.2037403583526611,
        #       -1.198288083076477, -1.194599986076355, -1.1915382146835327, -1.1856789588928223, -1.1837974786758423,
        #       -1.181101679801941, -1.1802221536636353, -1.1732022762298584, -1.168973684310913, -1.1653053760528564,
        #       -1.1607050895690918, -1.1576251983642578, -1.1561278104782104, -1.151627540588379, -1.1474699974060059,
        #       -1.1430503129959106, -1.141729474067688, -1.1367303133010864, -1.1333932876586914, -1.132659912109375,
        #       -1.1284173727035522, -1.1276335716247559, -1.1255370378494263, -1.122094750404358, -1.119180679321289,
        #       -1.1185388565063477, -1.1165679693222046, -1.1139044761657715, -1.1156463623046875, -1.1147181987762451,
        #       -1.112619161605835, -1.1116889715194702, -1.1118478775024414, -1.1122854948043823, -1.1127668619155884,
        #       -1.1139581203460693, -1.1125270128250122, -1.11636221408844, -1.1169090270996094, -1.1176172494888306,
        #       -1.1203768253326416, -1.121658444404602, -1.1238583326339722, -1.126365303993225, -1.1287829875946045,
        #       -1.132352590560913, -1.133167028427124, -1.1369041204452515, -1.1386027336120605, -1.1439541578292847,
        #       -1.148916244506836, -1.1515861749649048, -1.1546255350112915, -1.1600993871688843, -1.1645710468292236,
        #       -1.1678721904754639, -1.1713933944702148, -1.1772873401641846, -1.178864598274231, -1.1839996576309204,
        #       -1.1884450912475586, -1.191317081451416, -1.194790244102478, -1.1986033916473389, -1.2014787197113037,
        #       -1.2049907445907593, -1.2087657451629639, -1.2104215621948242, -1.2126774787902832, -1.216558814048767,
        #       -1.2189692258834839, -1.219964861869812, -1.2222758531570435, -1.2229005098342896, -1.2242646217346191,
        #       -1.2276397943496704, -1.2282586097717285, -1.2265111207962036, -1.2298611402511597, -1.228419542312622,
        #       -1.2327992916107178, -1.2328258752822876, -1.2327094078063965, -1.2333853244781494, -1.2328118085861206,
        #       -1.23393714427948, -1.2341960668563843, -1.2344295978546143, -1.234483242034912, -1.2340608835220337,
        #       -1.2356117963790894, -1.2346738576889038, -1.238371729850769, -1.2380892038345337, -1.2353495359420776,
        #       -1.2342487573623657, -1.2346986532211304, -1.2338769435882568, -1.2326098680496216, -1.2308186292648315,
        #       -1.231181263923645, -1.2299102544784546, -1.231500506401062, -1.2292640209197998, -1.2279962301254272,
        #       -1.227673888206482, -1.2305201292037964, -1.2301417589187622, -1.2306333780288696, -1.2314858436584473,
        #       -1.2301628589630127, -1.230540156364441, -1.2296648025512695, -1.2316631078720093, -1.2302508354187012,
        #       -1.2304672002792358, -1.2304397821426392, -1.2281960248947144, -1.2257317304611206, -1.2255254983901978,
        #       -1.2242456674575806, -1.2245012521743774, -1.222375750541687, -1.2211228609085083, -1.2205276489257812,
        #       -1.2165647745132446, -1.2155742645263672, -1.2156829833984375, -1.2133793830871582, -1.2116124629974365,
        #       -1.2104929685592651, -1.2084205150604248, -1.2095000743865967, -1.2046808004379272, -1.2013237476348877,
        #       -1.1991815567016602, -1.1968883275985718, -1.1941299438476562, -1.190078616142273, -1.1898534297943115,
        #       -1.1869781017303467, -1.1822361946105957, -1.1796317100524902, -1.1760517358779907, -1.1707712411880493,
        #       -1.1694817543029785, -1.167375922203064, -1.16276216506958, -1.1606920957565308, -1.15845787525177,
        #       -1.1555042266845703, -1.154378056526184, -1.1510910987854004, -1.145758032798767, -1.1424909830093384,
        #       -1.1393932104110718, -1.1381651163101196, -1.1345088481903076, -1.1318687200546265, -1.1275224685668945,
        #       -1.124463438987732, -1.1219199895858765, -1.119828462600708, -1.1199837923049927, -1.1165261268615723,
        #       -1.1171903610229492, -1.115247130393982, -1.1129741668701172, -1.1119508743286133, -1.1106829643249512,
        #       -1.1081087589263916, -1.1053873300552368, -1.1062201261520386, -1.1063086986541748, -1.104898452758789
        #       ]
        # y1 = y1[-192:]
        #
        # len_y1 = len(y1)  # 78
        # # FEDformer
        # y2 = [-1.0256134271621704, -1.0072129964828491, -0.9683144092559814, -1.007248878479004, -0.9973700046539307,
        #       -0.9744808077812195, -0.9884195923805237, -1.0312336683273315, -0.9992154836654663, -0.994556725025177,
        #       -0.9929692149162292, -0.9996421933174133, -0.9868636131286621, -0.9895613193511963, -0.9838409423828125,
        #       -1.020557165145874, -0.9883230924606323, -1.0077463388442993, -1.0170377492904663, -1.0575940608978271,
        #       -1.0419715642929077, -1.0686596632003784, -1.0769261121749878, -1.0948398113250732, -1.1231958866119385,
        #       -1.1499029397964478, -1.1189085245132446, -1.1154998540878296, -1.1045496463775635, -1.1065177917480469,
        #       -1.1296638250350952, -1.1584827899932861, -1.1880145072937012, -1.1983184814453125, -1.2011768817901611,
        #       -1.2169322967529297, -1.2387086153030396, -1.2389020919799805, -1.2619613409042358, -1.2529504299163818,
        #       -1.2515690326690674, -1.2632901668548584, -1.268833875656128, -1.252384066581726, -1.2680023908615112,
        #       -1.3059139251708984, -1.2929282188415527, -1.282692313194275, -1.2775444984436035, -1.3256278038024902,
        #       -1.326005220413208, -1.3456817865371704, -1.3568391799926758, -1.3673810958862305, -1.3669544458389282,
        #       -1.3466118574142456, -1.3163963556289673, -1.3404099941253662, -1.316084861755371, -1.3043826818466187,
        #       -1.2986299991607666, -1.289446473121643, -1.293558955192566, -1.2843270301818848, -1.2972832918167114,
        #       -1.2953366041183472, -1.2932851314544678, -1.3231340646743774, -1.3063850402832031, -1.3042054176330566,
        #       -1.301942229270935, -1.2914313077926636, -1.285255789756775, -1.290000557899475, -1.2794264554977417,
        #       -1.276307463645935, -1.2421343326568604, -1.2364944219589233, -1.2010260820388794, -1.207541823387146,
        #       -1.2090777158737183, -1.2067558765411377, -1.191481113433838, -1.1952954530715942, -1.203223466873169,
        #       -1.1784493923187256, -1.2049522399902344, -1.1707983016967773, -1.1673331260681152, -1.1473112106323242,
        #       -1.1498334407806396, -1.1320445537567139, -1.097331166267395, -1.0826855897903442, -1.0807595252990723,
        #       -1.0711772441864014, -1.0534297227859497, -1.065360188484192, -1.0579642057418823, -1.0401924848556519,
        #       -1.0527927875518799, -1.0333061218261719, -1.027043342590332, -0.9982144236564636, -1.007606029510498,
        #       -0.9903267025947571, -0.9877015948295593, -0.9883584976196289, -0.9691176414489746, -0.9639567732810974,
        #       -0.9490216970443726, -0.9423344135284424, -0.9439980387687683, -0.9293304681777954, -0.9250389337539673,
        #       -0.9223543405532837, -0.9128368496894836, -0.9160298705101013, -0.9191224575042725, -0.9243608713150024,
        #       -0.9260054230690002, -0.9483818411827087, -0.9409024119377136, -0.9230338335037231, -0.8938559889793396,
        #       -0.893379271030426, -0.8968873620033264, -0.8934271931648254, -0.8924617171287537, -0.8939651250839233,
        #       -0.9205252528190613, -0.907304048538208, -0.9215571880340576, -0.9180778861045837, -0.9248907566070557,
        #       -0.9116346836090088, -0.90995854139328, -0.9159867763519287, -0.9162512421607971, -0.915780782699585,
        #       -0.9398106336593628, -0.9445948004722595, -0.9169132709503174, -0.9150156378746033, -0.906676173210144,
        #       -0.9095101952552795, -0.9132826924324036, -0.9267863035202026, -0.9257246255874634, -0.921627938747406,
        #       -0.9324040412902832, -0.9301795959472656, -0.9473726153373718, -0.9442137479782104, -0.9325693845748901,
        #       -0.9371885061264038, -0.9315774440765381, -0.9441510438919067, -0.9528350234031677, -0.9622169733047485,
        #       -0.9792894721031189, -0.9878349304199219, -1.0365815162658691, -1.0133581161499023, -1.0317153930664062,
        #       -1.0330511331558228, -1.0432753562927246, -1.0734838247299194, -1.0805913209915161, -1.11277174949646,
        #       -1.088101863861084, -1.0773015022277832, -1.1152235269546509, -1.103106141090393, -1.1392430067062378,
        #       -1.1518137454986572, -1.1706565618515015, -1.2170958518981934, -1.232541799545288, -1.2330678701400757,
        #       -1.227673888206482, -1.247634768486023, -1.2311019897460938, -1.2373738288879395, -1.2126051187515259,
        #       -1.2375178337097168, -1.2296991348266602, -1.2277833223342896, -1.21734619140625, -1.2177420854568481,
        #       -1.1871806383132935, -1.18930983543396
        #       ]
        #
        # len_y2 = len(y2)  # 60
        # # Autoformer
        # y3 = [-1.1217060089111328, -1.1176087856292725, -1.1286040544509888, -1.1249836683273315, -1.1192578077316284,
        #       -1.1199008226394653, -1.1237214803695679, -1.108924388885498, -1.0992608070373535, -1.1172436475753784,
        #       -1.1236790418624878, -1.118556022644043, -1.1072903871536255, -1.0986298322677612, -1.0921885967254639,
        #       -1.092389702796936, -1.117046594619751, -1.1229609251022339, -1.125041127204895, -1.125533103942871,
        #       -1.124044418334961, -1.1229952573776245, -1.1236292123794556, -1.127947449684143, -1.125760793685913,
        #       -1.1255284547805786, -1.142282485961914, -1.1550203561782837, -1.1635948419570923, -1.1649158000946045,
        #       -1.1604368686676025, -1.1596691608428955, -1.1581764221191406, -1.1530510187149048, -1.1395673751831055,
        #       -1.13108491897583, -1.1264830827713013, -1.12746262550354, -1.130125880241394, -1.1339304447174072,
        #       -1.1229112148284912, -1.1061230897903442, -1.1117068529129028, -1.1151905059814453, -1.1160458326339722,
        #       -1.1141784191131592, -1.1152817010879517, -1.1128488779067993, -1.1124327182769775, -1.1160376071929932,
        #       -1.1247514486312866, -1.133724570274353, -1.1353065967559814, -1.1375012397766113, -1.1402027606964111,
        #       -1.14254629611969, -1.1449172496795654, -1.1468052864074707, -1.1359630823135376, -1.1458938121795654,
        #       -1.1515154838562012, -1.1606872081756592, -1.1596962213516235, -1.1571037769317627, -1.1379739046096802,
        #       -1.1230136156082153, -1.12850022315979, -1.1330807209014893, -1.1382774114608765, -1.1424742937088013,
        #       -1.1407939195632935, -1.1378874778747559, -1.132516622543335, -1.1207808256149292, -1.1089996099472046,
        #       -1.1057491302490234, -1.1067858934402466, -1.1064034700393677, -1.104668378829956, -1.1009373664855957,
        #       -1.0938276052474976, -1.0975626707077026, -1.1084342002868652, -1.1219416856765747, -1.1281101703643799,
        #       -1.1303424835205078, -1.1334079504013062, -1.1315574645996094, -1.1339893341064453, -1.1315609216690063,
        #       -1.130020260810852, -1.1256554126739502, -1.116907000541687, -1.1050300598144531, -1.098122239112854,
        #       -1.0928367376327515, -1.0892034769058228, -1.0836480855941772, -1.0765608549118042, -1.0716743469238281,
        #       -1.0666162967681885, -1.0664088726043701, -1.064516544342041, -1.0621100664138794, -1.059330940246582,
        #       -1.0573818683624268, -1.0524235963821411, -1.0495473146438599, -1.0488314628601074, -1.0494343042373657,
        #       -1.0539183616638184, -1.060245394706726, -1.0744118690490723, -1.0807554721832275, -1.0856492519378662,
        #       -1.087070107460022, -1.0839606523513794, -1.080146312713623, -1.0820696353912354, -1.0929397344589233,
        #       -1.0994240045547485, -1.106442928314209, -1.114691138267517, -1.1183937788009644, -1.1087126731872559,
        #       -1.1065868139266968, -1.1012226343154907, -1.1006078720092773, -1.1025296449661255, -1.1035243272781372,
        #       -1.1038198471069336, -1.1018718481063843, -1.1058785915374756, -1.106797456741333, -1.1178641319274902,
        #       -1.1152180433273315, -1.110554814338684, -1.0953620672225952, -1.093564510345459, -1.093187689781189,
        #       -1.0967743396759033, -1.101563572883606, -1.0899593830108643, -1.091446876525879, -1.1163979768753052,
        #       -1.1163442134857178, -1.108597993850708, -1.1093758344650269, -1.1099302768707275, -1.114704966545105,
        #       -1.1190152168273926, -1.1101820468902588, -1.1096680164337158, -1.1029456853866577, -1.099413514137268,
        #       -1.103232502937317, -1.1114559173583984, -1.1117615699768066, -1.1112679243087769, -1.1102944612503052,
        #       -1.0967206954956055, -1.0950376987457275, -1.1038565635681152, -1.107871651649475, -1.1142663955688477,
        #       -1.1192833185195923, -1.1135804653167725, -1.111865758895874, -1.102374792098999, -1.0933027267456055,
        #       -1.0856949090957642, -1.0863158702850342, -1.0876392126083374, -1.083505630493164, -1.0888313055038452,
        #       -1.0877586603164673, -1.0924701690673828, -1.0999841690063477, -1.1049407720565796, -1.1130927801132202,
        #       -1.1194615364074707, -1.1200331449508667, -1.1188371181488037, -1.117217779159546, -1.1077455282211304,
        #       -1.1092873811721802, -1.1123172044754028, -1.1150070428848267, -1.1180633306503296, -1.1142016649246216,
        #       -1.1048760414123535, -1.106879711151123
        #       ]
        # len_y3 = len(y3)  # 60
        # true = torch.Tensor(true).view(1, 192, 1).repeat(32, 1, 7)
        # ACDN = torch.Tensor(y1).view(1, 192, 1).repeat(32, 1, 7)
        # FEDformer = torch.Tensor(y2).view(1, 192, 1).repeat(32, 1, 7)
        # Autoformer = torch.Tensor(y3).view(1, 192, 1).repeat(32, 1, 7)
        #
        # res, trend = self.decompose(true)
        # ACDN_res, ACDN_trend = self.decompose(ACDN)
        # FEDformer_res, FEDformer_trend = self.decompose(FEDformer)
        # Autoformer_res, Autoformer_trend = self.decompose(Autoformer)
        #
        # draw_trend_res(trend, ACDN_trend, FEDformer_trend, Autoformer_trend, 'trend')
        # draw_trend_res(res, ACDN_res, FEDformer_res, Autoformer_res, 'seasonal')

        return output_final, pred_mean, pred_std, label_pred_mean[:, -self.pred_len:, :], output, output_linear # [B, L, D]




